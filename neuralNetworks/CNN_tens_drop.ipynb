{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "from torchvision.transforms import Normalize\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device for computation (CPU or GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CNN_Mask_to_Tens import CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module): # results in accuracy of 73.22%\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(256 * 2 * 2, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.fc5 = nn.Linear(128, 1)\n",
    "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.leaky_relu(self.conv1(x)))\n",
    "        x = self.pool(self.leaky_relu(self.conv2(x)))\n",
    "        x = self.pool(self.leaky_relu(self.conv3(x)))\n",
    "        x = self.pool(self.leaky_relu(self.conv4(x)))\n",
    "        x = self.pool(self.leaky_relu(self.conv5(x)))\n",
    "        x = x.view(-1, 256 * 2 * 2)\n",
    "        x = self.leaky_relu(self.fc1(x))\n",
    "        x = self.leaky_relu(self.fc2(x))\n",
    "        x = self.leaky_relu(self.fc3(x))\n",
    "        x = self.leaky_relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training dataset\n",
    "# Set up the data directories\n",
    "sets = ['Train_510','Train_1020','Train_2040','Train_4080','Train_8160','Train_10200']\n",
    "for item in sets:\n",
    "    data_dir = 'Test_Images'\n",
    "    train_dir = os.path.join(data_dir, item)\n",
    "\n",
    "    # Define a function to get the labels from the image filenames\n",
    "    def get_label(filename):\n",
    "        match = re.search(r'\\d+\\.?\\d*', filename)\n",
    "        if match:\n",
    "            return float(match.group())\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # Define a list to store the image filenames and labels\n",
    "    train_data = []\n",
    "\n",
    "    # Iterate over the training images and add them to the list\n",
    "    for filename in os.listdir(train_dir):\n",
    "        label = get_label(filename)\n",
    "        if label is not None:\n",
    "            train_data.append([os.path.join(train_dir, filename), label])\n",
    "\n",
    "    # Convert the list to a dataframe\n",
    "    train_df = pd.DataFrame(train_data, columns=['filename', 'label'])\n",
    "\n",
    "    # Save the dataframe to a CSV file\n",
    "    train_df.to_csv(os.path.join(data_dir, item+'.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, csv_file, image_folder, transform=None):\n",
    "        self.data = self._load_data(csv_file)\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path, label = self.data[index]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def _load_data(self, csv_file):\n",
    "    # Load and preprocess data from the CSV file\n",
    "    # Example: Assuming the CSV file has two columns representing image paths and labels\n",
    "        data = []\n",
    "        with open(csv_file, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            for line in lines[1:]:  # Skip the header row\n",
    "                # Split the line by comma or any other appropriate delimiter\n",
    "                image_path, label = line.strip().split(',')\n",
    "                data.append((image_path, float(label)))  # Parse the label as float\n",
    "        return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = Image.open('Test_Images\\Train_8160\\drop_s30_v5_r0.5_str6_pos0_cam2.png')  # Replace 'input_image.jpg' with the path to your input image file\n",
    "transform = ToTensor()\n",
    "tensor = transform(input_image)\n",
    "# Calculate mean and standard deviation\n",
    "mean = torch.mean(tensor)\n",
    "std = torch.std(tensor)\n",
    "\n",
    "# Normalize the data\n",
    "normalizer = Normalize(mean=[mean.item(), mean.item(), mean.item()], std=[std.item(), std.item(), std.item()])\n",
    "# Set up the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(64),\n",
    "    transforms.CenterCrop(64),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean= [mean, mean, mean], std=[std, std, std])               # Need to be adjusted, acording to dataset\n",
    "])\n",
    "\n",
    "# Define the transformation for binarizing the inputs\n",
    "threshold = 0.5  # Set your desired threshold value\n",
    "binarize_transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: torch.where(x < threshold, torch.tensor(0.), torch.tensor(1.))),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "csv_file = 'Test_Images/Train_10200.csv'  # Replace with the path to your CSV file\n",
    "image_folder = 'Test_Images/Train_10200'  # Replace with the path to your image folder\n",
    "\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "dataset = CustomImageDataset(csv_file, image_folder, transform=transform)\n",
    "# Define the sizes for your training, validation, and test sets\n",
    "train_size = int(0.6 * len(dataset))  # 60% for training\n",
    "val_size = int(0.2 * len(dataset))    # 20% for validation\n",
    "test_size = len(dataset) - train_size - val_size  # Remaining 20% for test\n",
    "\n",
    "# Use random_split to split the dataset into training, validation, and test sets\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create data loaders for training and validation sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the loss function and optimizer\n",
    "# model = CNN()\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training counstants\n",
    "num_epochs = 30\n",
    "best_val_loss = float('inf')\n",
    "best_learning_rate = None\n",
    "patience = 5  # Number of epochs to wait for improvement\n",
    "counter = 0  # Counter to track the number of epochs without improvement\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Create empty lists\n",
    "train_losses = []  # List to store training losses\n",
    "val_losses = []  # List to store validation losses\n",
    "epoch_numbers = [] # List to store epoch numbers\n",
    "\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "model = CNN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        model.train()\n",
    "        for i, (inputs, labels) in enumerate(train_loader, 0):\n",
    "            optimizer.zero_grad()\n",
    "            inputs = inputs.to(device)  # Move inputs to the device\n",
    "            labels = labels.to(device)  # Move labels to the device\n",
    "\n",
    "             # Binarize inputs\n",
    "            threshold = 0.5\n",
    "            inputs = torch.where(inputs >= threshold, torch.ones_like(inputs), torch.zeros_like(inputs))\n",
    "            inputs = inputs[:, :1, :, :]\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            labels = labels.unsqueeze(1)\n",
    "            loss = criterion(outputs, labels.float())\n",
    "\n",
    "            l1_lambda = 0.01\n",
    "            l1_regularization = torch.tensor(0.).to(device)\n",
    "            for param in model.parameters():\n",
    "                l1_regularization += torch.norm(param, 1).to(device)\n",
    "            loss += l1_lambda * l1_regularization\n",
    "\n",
    "            l2_lambda = 0.01\n",
    "            l2_regularization = torch.tensor(0.).to(device)\n",
    "            for param in model.parameters():\n",
    "                l2_regularization += torch.norm(param, 2).to(device)\n",
    "            loss += l2_lambda * l2_regularization\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % 100 == 99:\n",
    "                print('[%d, %5d] loss: %.6f' % (epoch + 1, i + 1, running_loss / 100))\n",
    "                running_loss = 0.0\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0.0\n",
    "            val_samples = 0\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(device)  # Move inputs to the device\n",
    "                labels = labels.to(device)  # Move labels to the device\n",
    "\n",
    "                # Binarize inputs\n",
    "                threshold = 0.5\n",
    "                inputs = torch.where(inputs >= threshold, torch.ones_like(inputs), torch.zeros_like(inputs))\n",
    "                inputs = inputs[:, :1, :, :]\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                labels = labels.unsqueeze(1)\n",
    "                loss = criterion(outputs, labels.float())\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                val_samples += inputs.size(0)\n",
    "\n",
    "            average_val_loss = val_loss / val_samples\n",
    "\n",
    "            train_losses.append(running_loss / len(train_loader))\n",
    "            val_losses.append(average_val_loss)\n",
    "\n",
    "            epoch_numbers.append(epoch + 1)\n",
    "            print('Epoch %d - Training Loss: %.6f - Validation Loss: %.6f - Learning Rate: %.3f' % (epoch + 1, running_loss / len(train_loader), average_val_loss, learning_rate))\n",
    "\n",
    "        if average_val_loss < best_val_loss:\n",
    "            best_val_loss = average_val_loss\n",
    "            counter = 0\n",
    "            torch.save(model.state_dict(), f'trained_model_{item}.pt')\n",
    "\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print('No improvement in validation loss. Early stopping.')\n",
    "                break\n",
    "\n",
    "        print('Epoch %d completed.' % (epoch + 1))\n",
    "\n",
    "# Plotting the losses\n",
    "# epochs = range(1, num_epochs + 1)\n",
    "plt.plot(epoch_numbers, train_losses, label='Training Loss')\n",
    "plt.plot(epoch_numbers, val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('trained_model')  # Replace 'trained_model.pth' with the path to your trained model file\n",
    "\n",
    "# Load and preprocess the input image\n",
    "input_image = Image.open('Test_Images\\Train\\drop_s30_v5_r0.5_str6_pos0_cam79.png')  # Replace 'input_image.jpg' with the path to your input image file\n",
    "input_image = input_image.convert('RGB')\n",
    "input_tensor = transform(input_image)\n",
    "\n",
    "# input_tensor = torchvision.transforms.ToTensor()(input_image).unsqueeze(0)  # Preprocess the image and add a batch dimension\n",
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()  # Replace YourModel with your actual model class\n",
    "# Load the trained weights\n",
    "model.load_state_dict(torch.load('trained_model'))\n",
    "\n",
    "# Access the weights\n",
    "weights = model.state_dict()\n",
    "\n",
    "# Print the weights\n",
    "for name, param in weights.items():\n",
    "    print(f'Weights:\\n{param}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Make a prediction\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)\n",
    "\n",
    "# Convert the output to a readable format\n",
    "predicted_value = output.item()\n",
    "\n",
    "# Print the predicted value\n",
    "print('Predicted value:', predicted_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device for computation (CPU or GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create a DataFrame to store the predictions\n",
    "predictions_df = pd.DataFrame(columns=['Image', 'Real Value', 'Predicted Value', 'Difference'])\n",
    "\n",
    "# Iterate over the figures in the folder\n",
    "for filename in os.listdir(image_folder):\n",
    "    if filename.endswith('.jpg') or filename.endswith('.png'):  # Adjust the file extensions as per your figures\n",
    "        image_path = os.path.join(image_folder, filename)\n",
    "        \n",
    "        # Load and preprocess the image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Extract the real value from the filename\n",
    "        real_value = int(filename.split('_')[1][1:])  # Adjust the splitting pattern to extract the desired value\n",
    "        \n",
    "        # Make the prediction\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "        predicted_value = output.item()\n",
    "        \n",
    "        # Calculate the difference between predicted and real value\n",
    "        difference = predicted_value - real_value\n",
    "\n",
    "        # Add the prediction, real value, and difference to the DataFrame\n",
    "        predictions_df = predictions_df.append({'Image': filename, 'Real Value': real_value, 'Predicted Value': predicted_value, 'Difference': difference}, ignore_index=True)\n",
    "\n",
    "# Print the predictions table\n",
    "print(predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the real and predicted values from the DataFrame\n",
    "real_values = predictions_df['Real Value']\n",
    "predicted_values = predictions_df['Predicted Value']\n",
    "\n",
    "# Calculate the mean squared error (MSE)\n",
    "differences = predictions_df['Difference']\n",
    "mse = np.mean(differences ** 2)\n",
    "\n",
    "# Print MSE\n",
    "print('Mean Squared Error (MSE):', mse)\n",
    "\n",
    "# Plot the predicted values and the real values\n",
    "plt.plot(predicted_values, label='Predicted Values')\n",
    "plt.plot(real_values, label='Real Values')\n",
    "plt.xlabel('Figure Index')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Predicted Values vs Real Values')\n",
    "plt.legend()\n",
    "\n",
    "# Display the MSE in the plot\n",
    "plt.text(0.5, 0.95, f'MSE: {mse:.2f}', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the MSE per real value\n",
    "mse_per_real_value = predictions_df.groupby('Real Value')['Difference'].apply(lambda x: mean_squared_error(x, np.zeros_like(x)))\n",
    "\n",
    "# Print the MSE per real value\n",
    "print('MSE per Real Value:')\n",
    "print(mse_per_real_value)\n",
    "\n",
    "# Plot the MSE per real value\n",
    "plt.bar(mse_per_real_value.index, mse_per_real_value)\n",
    "plt.xlabel('Real Value')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('MSE per Real Value')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to store the predictions\n",
    "predictions_test_df = pd.DataFrame(columns=['Real Value', 'Predicted Value', 'Difference'])\n",
    "\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize variables for tracking correct predictions and total samples\n",
    "correct_predictions = 0\n",
    "total_samples = 0\n",
    "\n",
    "# Define the range for considering a prediction as correct\n",
    "max_range = 0.5\n",
    "\n",
    "# Iterate over the test data\n",
    "for sample in test_loader:\n",
    "\n",
    "    # Move the input data to the device\n",
    "    inputs = sample[0].to(device)  # Assuming the input images are the first element in each sample\n",
    "    labels = sample[1].to(device)  # Assuming the labels are the second element in each sample\n",
    "\n",
    "    # Forward pass through the model\n",
    "    with torch.no_grad():\n",
    "        predicted_values = model(inputs).squeeze().tolist()\n",
    "\n",
    "    # Get the predicted labels\n",
    "    predicted_labels = outputs  # Assuming the model output is a single scalar value\n",
    "\n",
    "    # Calculate the number of correct predictions within the range\n",
    "    # correct_predictions += ((predicted_labels >= min_range) & (predicted_labels <= max_range) & (labels >= min_range) & (labels <= max_range)).sum().item()\n",
    "    \n",
    "# Iterate over the predicted values and add them to the DataFrame\n",
    "    for i in range(len(predicted_values)):\n",
    "        real_value = labels[i].item()\n",
    "        predicted_value = predicted_values[i]\n",
    "\n",
    "        # Check if the predicted value is within the desired range (40-70)\n",
    "        if real_value >= 40 and real_value <= 70:\n",
    "            # Calculate the difference between predicted and real value\n",
    "            difference = predicted_value - real_value\n",
    "\n",
    "            # Add the prediction, real value, and difference to the DataFrame\n",
    "            predictions_test_df = predictions_test_df.append({'Real Value': real_value, 'Predicted Value': predicted_value, 'Difference': difference}, ignore_index=True)\n",
    "\n",
    "            # Check if the prediction is correct within the desired range\n",
    "            if abs(real_value - predicted_value) <= max_range:\n",
    "                correct_predictions += 1\n",
    "\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = correct_predictions / (len(predictions_test_df)-1)\n",
    "\n",
    "# Print the predictions DataFrame\n",
    "print(predictions_test_df)\n",
    "\n",
    "# Print the accuracy\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the predictions_test_df to include only the range 40-70 real values $$$\n",
    "filtered_predictions_test_df = predictions_test_df[(predictions_test_df['Real Value'] >= 40) & (predictions_test_df['Real Value'] <= 70)]\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.scatter(filtered_predictions_test_df['Real Value'], filtered_predictions_test_df['Predicted Value'], marker='x')\n",
    "\n",
    "# Plot the line y = x\n",
    "x = np.linspace(filtered_predictions_test_df['Real Value'].min(), filtered_predictions_test_df['Real Value'].max(), 100)\n",
    "plt.plot(x, x, color='orange')\n",
    "\n",
    "# Set plot labels and title\n",
    "plt.xlabel('Real Value')\n",
    "plt.ylabel('Predicted Value')\n",
    "plt.title('Scatter Plot of Real Value vs Predicted Value')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the MSE per real value\n",
    "mse_per_real_value_test = filtered_predictions_test_df.groupby('Real Value')['Difference'].apply(lambda x: mean_squared_error(x, np.zeros_like(x)))\n",
    "\n",
    "# Plot the MSE per real value\n",
    "plt.bar(mse_per_real_value_test.index, mse_per_real_value_test)\n",
    "plt.xlabel('Real Value')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('MSE per Real Value')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
